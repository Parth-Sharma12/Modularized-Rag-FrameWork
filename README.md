# ğŸ§© Modular RAG Visualizer  
### Understanding why Vector Databases alone are not enough

ğŸ”— **Live Demo:** [[Add Demo Link Here](https://drive.google.com/drive/folders/1AeOBonHmU9iy4q4-5UdeerngSCi0Nx3j?usp=drive_link)]  


---

## ğŸ“Œ Overview

Large Language Models (LLMs) often **hallucinate** â€” not because they lack intelligence, but because they are forced to answer questions **without sufficient or well-structured context**.

This project is a **fully visual, modular Retrieval-Augmented Generation (RAG) demo** that shows â€” step by step â€” **where hallucinations originate and how modular RAG reduces them**.

Instead of treating RAG as a black box, this demo focuses on:
- Explainability
- Visualization
- Component-level reasoning

---

## ğŸ§  What This Demo Demonstrates

### ğŸ”µ 1. Vector Database Retrieval
- Fast semantic similarity search
- High recall but noisy results
- Visualized using similarity score charts

### ğŸ§  2. Vector Space Visualization (PCA)
- PCA projection of embeddings
- Shows:
  - All document chunks
  - Retrieved chunks
  - Query embedding
- Explains why **similarity â‰  relevance**

### ğŸŸ¢ 3. Cross-Encoder Re-ranking
- High-precision semantic scoring
- Improves relevance
- Exposes limitations for **multi-part questions**

### ğŸŸ£ 4. Context Construction
- Carefully selected and ordered chunks
- Demonstrates that **context quality is the real bottleneck**

### ğŸ¤– 5. Answer Generation
- Final grounded response using Gemini
- Reduced hallucinations due to better context

### ğŸ”½ Noise Reduction Funnel
- Visual funnel showing chunk reduction across RAG stages

---

## âœ¨ Key Learnings

- Vector databases are necessary, but **not sufficient**
- Cross-encoders improve relevance, but donâ€™t fully understand intent
- Hallucinations often result from:
  - Missing context
  - Poor chunk selection
  - Weak context construction
- **RAG is a system-design problem, not a tooling problem**

---

## ğŸ—ï¸ Modular Architecture

